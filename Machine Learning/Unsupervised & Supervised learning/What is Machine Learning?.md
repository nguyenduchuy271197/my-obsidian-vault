Supervised learning:
Unsupervised learning
Recommend systems
Reinforcement learning
Square error cost function (Cost function)
Slope
Gradient Descent - "Batch" gradient descent -> train all data in one epoch
Learning rate
Derivative
Assignment & Truth assertion
Repeat until convergence
Simultanously update
How to choose learning rate (alpha) ?
- small -> GD is slow
- large -> overshoot, never reach minimum, diverge

convex function - bowl shape -> only one global minimum



Parameters: w and b (weights or coefficients)
Hyperparameters: learning rate

![[Screen Shot 2022-06-30 at 22.11.13.png]]![[Screen Shot 2022-06-30 at 22.36.15.png]]